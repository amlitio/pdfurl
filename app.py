
from flask import Flask, request
from flask_cors import CORS
from dotenv import load_dotenv
from langchain.document_loaders import PyPDFLoader
from langchain.embeddings import CohereEmbeddings
from langchain.vectorstores import Qdrant
from langchain.llms import OpenAI
from qdrant_client import QdrantClient
import os

# Loading environment variables
load_dotenv()

# Getting API keys and URLs from environment variables
openai_api_key = os.environ.get('openai_api_key')
cohere_api_key = os.environ.get('cohere_api_key')
qdrant_url = os.environ.get('qdrant_url')
qdrant_api_key = os.environ.get('qdrant_api_key')

# Flask config
app = Flask(__name__)
CORS(app)

# Embedding endpoint
@app.route('/embed')
def embed_pdf():
    # Get PDF file URL from query parameter
    file_url = request.args.get('url')

    # Load and split PDF file into individual pages
    loader = PyPDFLoader(file_url)
    docs = loader.load_and_split()

    # Generate embeddings for each page using Cohere
    embeddings = CohereEmbeddings(model='multilingual-22-12', cohere_api_key=cohere_api_key)
    embeddings_dict = embeddings.embed_documents(docs)

    # Create Qdrant vector store for embeddings
    collection_name = 'pdf_embeddings'
    qdrant = Qdrant.from_embeddings(embeddings_dict, url=qdrant_url, collection_name=collection_name, prefer_grpc=True, api_key=qdrant_api_key)

    # Return name of created collection
    return {"collection_name": qdrant.collection_name}

# Retrieve endpoint
@app.route('/retrieve')
def retrieve_info():
    # Get collection name and query from query parameters
    collection_name = 'pdf_embeddings'
    query = request.args.get('query')

    # Retrieve documents similar to query from Qdrant
    client = QdrantClient(url=qdrant_url, prefer_grpc=True, api_key=qdrant_api_key)
    embeddings = CohereEmbeddings(model='multilingual-22-12', cohere_api_key=cohere_api_key)
    qdrant = Qdrant(client=client, collection_name=collection_name, embedding_function=embeddings.embed_query)
    search_results = qdrant.similarity_search(query, k=3)

    # Generate answer using pre-trained question answering model from OpenAI
    chain = load_qa_chain(OpenAI(openai_api_key, temperature=0.2), chain_type='stuff')
    results = chain({"input_documents": search_results, "question": query}, return_only_outputs=True)

    # Return answer generated by model
    return {"results": results["output_text"]}

if __name__ == '__main__':
    app.run(debug=True)
